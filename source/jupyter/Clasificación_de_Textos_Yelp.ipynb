{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación de Textos - Yelp! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este Notebook se realiza una clasificación de textos del dataset de [Yelp!](https://www.yelp.com/dataset).  \n",
    "De este dataset, se utiliza el fichero *review.json* el cual contiene el conjunto de opiniones de los distintos clientes y el número de \"estrellas\" asignados. El objetivo es a partir del texto poder predecir el número de estrellas a asignar, lo cual corresponde a una clasificación basada en el sentimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar se importan las librerías necesarias para la realización de este trabajo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQpYtkGg2LuI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import logging\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1123,
     "status": "ok",
     "timestamp": 1556474814858,
     "user": {
      "displayName": "Javier de la Rosa Fernández",
      "photoUrl": "https://lh3.googleusercontent.com/-g34fDmLWAXU/AAAAAAAAAAI/AAAAAAAAXmE/hc7SWa5zatU/s64/photo.jpg",
      "userId": "06951023215962849709"
     },
     "user_tz": -120
    },
    "id": "07mg7_CT2Tf8",
    "outputId": "dd32b4d6-dfca-4a3a-97c6-9daa0fefd283"
   },
   "source": [
    "Se realiza le pre-procesamiento de la información, el cual está dividido en:\n",
    "1. Lectura del dataset\n",
    "2. Selección de variables\n",
    "3. Eliminación de catacteres especiales y paso a *lowercase*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 918,
     "status": "ok",
     "timestamp": 1556474817329,
     "user": {
      "displayName": "Javier de la Rosa Fernández",
      "photoUrl": "https://lh3.googleusercontent.com/-g34fDmLWAXU/AAAAAAAAAAI/AAAAAAAAXmE/hc7SWa5zatU/s64/photo.jpg",
      "userId": "06951023215962849709"
     },
     "user_tz": -120
    },
    "id": "u2Eg9SUc2ibp",
    "outputId": "2c9b1410-410e-454b-eb69-e4db440b8c4a"
   },
   "outputs": [],
   "source": [
    "root_path = 'yelp_dataset/review.json'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "data = pd.read_json(root_path_windows, lines=True)\n",
    "data = data[[\"text\", \"stars\"]]\n",
    "data[\"text\"] = data[\"text\"].str.replace('[^a-zA-Z]',' ').str.lower()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, se realiza un split en datos de entrenamiento y datos de test (80% - 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['text'],data['stars'], test_size = 0.2)\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muestreo\n",
    "Debido a que se va a analizar el comportamiento de los algoritmos con el aumento del tamaño muestral, también se realiza un meustreo de los datos leidos a aprtir de la selección de un porcentaje del dataset. Para ello se seleccionan los siguientes porcentajes: 10%, 30%, 50%, 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "data2 = data.head(int(len(data)*0.1))\n",
    "x_train_10, x_test_10, y_train_10, y_test_10 = train_test_split(data2['text'],data2['stars'], test_size = 0.2)\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "data3 = data.head(int(len(data)*0.3))\n",
    "x_train_30, x_test_30, y_train_30, y_test_30 = train_test_split(data3['text'],data3['stars'], test_size = 0.2)\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "data5 = data.head(int(len(data)*0.5))\n",
    "x_train_50, x_test_50, y_train_50, y_test_50 = train_test_split(data5['text'],data5['stars'], test_size = 0.2)\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "data75 = data.head(int(len(data)*0.75))\n",
    "x_train_75, x_test_75, y_train_75, y_test_75 = train_test_split(data75['text'],data75['stars'], test_size = 0.2)\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(x_train_10[\"data\"])\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "start = time.time()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(x_train_10)\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización del texto\n",
    "En el siguiente apartado se procede a extraer las *features* a partir del texto. Para ello se usan los algoritmos de CountVectorizer y TfidfVectorizer. Si bien estos algoritmos luego se incluyen en el Pipeline definido para el entrenamiento de los modelos estadísticos, este paso se realiza para obtener los tiempos arrojados por estos algoritmos ya que el pipeline no permite obtenerlo.\n",
    "\n",
    "Además, aunque esto no aparezca, se ha realizado para cada una de las variables muestrales almacenadas en el apartado anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(x_train)\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "start = time.time()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(x_train)\n",
    "end = time.time()\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de algoritmos no distribuidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8Tr5GiqlZ2Q"
   },
   "source": [
    "#### Multinominal NB\n",
    "Para este entrenamiento se hace uso de un pre-procesamiento de la información mediante: CountVector + TF-IDF.  \n",
    "Para su entrenamiento, se hace uso del Pipeline proporcionado por Scikit Learn que permite aplicar distintas funciones de manera secuencial, haciendo que el desarrollo sea más fácil de entender.  \n",
    "Aunque este entrenamiento se ha realizado para cada uno de los tamaños muestrales definidos previamente, únicamente se muestra el entrenamiento con el 100% de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1389,
     "status": "ok",
     "timestamp": 1556467565824,
     "user": {
      "displayName": "Javier de la Rosa Fernández",
      "photoUrl": "https://lh3.googleusercontent.com/-g34fDmLWAXU/AAAAAAAAAAI/AAAAAAAAXmE/hc7SWa5zatU/s64/photo.jpg",
      "userId": "06951023215962849709"
     },
     "user_tz": -120
    },
    "id": "J88sGFVo4NxU",
    "outputId": "ecfb156f-c5ff-46e1-d168-e747517fbf3a"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer()), \n",
    "    ('tfidf',TfidfTransformer()), \n",
    "    ('classifier',MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "y_pred = pipeline.predict(x_test)\n",
    "print('Accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Seconds: \", end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWk2W_pToHQD"
   },
   "source": [
    "#### Logistic Regression\n",
    "Para este entrenamiento se hace uso de un pre-procesamiento de la información mediante: CountVector + TF-IDF.  \n",
    "Para su entrenamiento, se hace uso del Pipeline proporcionado por Scikit Learn que permite aplicar distintas funciones de manera secuencial, haciendo que el desarrollo sea más fácil de entender.  \n",
    "Aunque este entrenamiento se ha realizado para cada uno de los tamaños muestrales definidos previamente, únicamente se muestra el entrenamiento con el 100% de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression()),\n",
    "               ])\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Seconds: \", end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCoLj6QDsLQO"
   },
   "source": [
    "#### Random Forest\n",
    "\n",
    "Para este entrenamiento se hace uso de un pre-procesamiento de la información mediante: CountVector + TF-IDF.  \n",
    "Para su entrenamiento, se hace uso del Pipeline proporcionado por Scikit Learn que permite aplicar distintas funciones de manera secuencial, haciendo que el desarrollo sea más fácil de entender.  \n",
    "Aunque este entrenamiento se ha realizado para cada uno de los tamaños muestrales definidos previamente, únicamente se muestra el entrenamiento con el 100% de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "start = time.time()\n",
    "pipeline = Pipeline([('bow', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('classifier', RandomForestClassifier(n_estimators=20)),\n",
    "               ])\n",
    "pipeline.fit(x_train, y_train)\n",
    "end = time.time()\n",
    "y_pred = pipeline.predict(x_test)\n",
    "print('Accuracy: %s' % accuracy_score(y_pred, y_test))\n",
    "print(\"Seconds: \", end - start)\n",
    "print(\"-------------------------------------------\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
